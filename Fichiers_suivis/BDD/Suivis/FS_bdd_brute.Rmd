---
title: "Création de la base de données"
author: "Jean-Baptiste Paroissien"
date: "26/01/2017"
output:
  github_document:
    dev: png
    fig_height: 10
    fig_width: 10
    md_extensions: +autolink_bare_uris+hard_line_breaks+header_attributes+line_blocks+table_captions
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
# Importation des paramètres de travail
source("/media/sf_GIS_ED/Dev/Scripts/master/Fonctions/R/importparametres.R")
repmaster <- "/media/sf_GIS_ED/Dev/Scripts/master/"
repdata <- "/media/sf_GIS_ED/Dev/Data/"
importparametres(repmaster=repmaster,repdata=repdata,dsn="PG:dbname='sol_elevage' host='localhost' port='5432' user='jb'")
```

# Objectifs

L'objectif de ce document est de centraliser dans un même fichier la création et la description d'une base de données pour étudier l'impact de l'élevage sur la qualité des sols. Cette base contient plusieurs types de données en lien avec les données pédologiques, les données associées à l'occupation du sol et les données environnementales générales telles que le climat ou la topographie. Le document décrit dans un premier temps l'architecture choisie et l'organisation des données. Il présente dans un second temps les scripts d'intégration des données. 

# L'architecture et l'organisation de la base de données

## Architecture technique

L'ensemble des données est stocké dans une base de données type postgresql/postgis. Le serveur de la base est en local et des conversions seront réalisées régulièrement vers une base SQLite/Spatialite pour faciliter le partage des données. SQLite diffère de la plupart des systèmes de gestion de base de données par la gestion d'un fichier de base directement sur le disque dur. A la différence de postgresql/postgis, il ne nécessit pas la création d'un serveur, ce qui facilite les échanges. Plus d'infos, [ici](http://www.developpez.com/actu/94614/Un-developpeur-evoque-cinq-raisons-pour-vous-faire-utiliser-SQLite-en-2016-que-pensez-vous-de-ses-arguments/).

## Organisation de la base de données

Deux types de données sont intégrées dans la base : les données brutes et les données élaborées

### Les données brutes

Elles proviennent de différentes sources et représentent des données qui ne sont pas directement exploitables pour des traitements statistiques ou de la cartographie. Elles sont stockées dans des schémas portant le nom de leur thématique associées (voir tableau ci-dessous). Ces données sont traitées et stockées dans des data marts.

```{r,evaL=TRUE,echo=FALSE}
# Description des tables (voir pour rajouter les métadonnées dans les commentaires de table)
metatable <- read.csv(paste(repmetadonnees,"Catalogue_schema.csv",sep=""),sep=";")
pander(metatable[metatable$Type %in% "Brute",][,c("Schéma","Description")])
```

### Les data marts

Les data marts (magasins de données) sont les données qui sont directement utilisables dans des traitements. Ces data marts se présentent sous la forme d'une ou plusieurs tables classées dans des schémas distincts. Le nom de ces schémas est préfixé par « dm_». Le tableau suivant dresse la liste des data marts disponibles :

```{r,evaL=TRUE,echo=FALSE}
metatable <- read.csv(paste(repmetadonnees,"Catalogue_schema.csv",sep=""),sep=";")
pander(metatable[metatable$Type %in% "Data_mart",][,c("Schéma","Description")])
```

## Documentation

La documentation de cette base de données s'articule autours d'un document général et de plusieurs modes opératoires destinés aux différents types d'utilisateurs (administrateur, développeur et utilisateur). Ces documents sont présents sur la forge du projet à cette [adresse](https://github.com/Rosalien/GISEDSol/tree/master/Documentation).

# Création de la base

La création de la base postgresql/postgis est réalisée selon la procédure suivante :

```{r, engine='bash',highlight=TRUE,eval=FALSE}
sudo -i -u postgres
postgres=# CREATE USER jb;
postgres-# ALTER ROLE jb WITH CREATEDB;
postgres=# CREATE DATABASE SOL_ELEVAGE OWNER jb;
postgres-# ALTER USER jb WITH ENCRYPTED PASSWORD '******';
postgres=# CREATE DATABASE sol_elevage OWNER jb;
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
```    

Configuration du fichier `\etc\odbc.ini` pour se connecter à la base avec R grâce au paquet RODBC.

```{r, engine='bash',highlight=TRUE,eval=FALSE}
[ODBC]
InstallDir = /usr/lib

[solelevage]
Driver = /usr/lib/x86_64-linux-gnu/odbc/psqlodbcw.so
Database = sol_elevage
Servername = localhost
Username = jb	
Password = *******
Port=5432
Protocol = 8.1
ReadOnly = 0
```

# Intégration des données de travail

| Catégorie                             | Type de données                     | Source                         | Contacts                                     |
|---------------------------------------|-------------------------------------|--------------------------------|----------------------------------------------|
| Données sols                          | Raster et ShapeFiles (L93)          | InfoSol                        | infosol@inra.fr                              |
| Données d'occupation du sol           | .xls, raster (L93)                  | RA, Corine Land Cover          | AGRESTE + christophe.perrot@idele.fr         |
| Données MAFOR                         | ShapeFiles (L93)                    | Interne IDELE                  | chritophe.perrot@idele.fr                    |
| Données administratives               | ShapeFiles (L93)                    | IGN + Agreste                  |                                              |


## Caractéristiques techniques

```{r, tidy=FALSE,eval=TRUE}
Sys.Date()
sessionInfo()
```

## Les données de bases

Les données dites de "bases" concernent les données administratives utilisées pour agréger certaines variables environnementales. Dans ce travail, les contours des communes, des cantons et des petites régions agricoles sont les principales échelles de travail utilisées. Elles ont été récupérées de la manière suivante :

- Communes et Canton : La version de 2011 de base de données [Geofla](http://professionnels.ign.fr/geofla) est utilisée. Cette version a été choisie pour être en accord avec les données du dernier recencement agricole (2010)
- Petite région agricole : Les données associées aux petites régions agricoles proviennent de [l'agreste](http://agreste.agriculture.gouv.fr/IMG/zip/comm-ra-pra2007.zip) et sont jointes aux données Geofla.

```{r,highlight=TRUE,eval=FALSE}

schema <- "dm_vecteurs"#nom du schéma pour intégrer les vecteurs

# Intégration des données communales et cantonnales dans la base
for(i in c("DEPARTEMENTS","CANTON","COMMUNE")){
  # Téléchargement
  URL <- paste("https://wxs-telechargement.ign.fr/oikr5jryiph0iwhw36053ptm/telechargement/inspire/GEOFLA_THEME-",i,"_2011_GEOFLA_1-1_SHP_LAMB93_FR-ED111/file/GEOFLA_1-1_SHP_LAMB93_FR-ED111.7z",sep="")
  system(paste("wget -P ",repBase," ",URL,sep=""))  

    # Décompression
  system(paste("7z e ",repBase,"GEOFLA_1-1_SHP_LAMB93_FR-ED111.7z -o",repBase," -y",sep=""))
  
  # Intégration dans la BDD (suppression si le vecteur exsite déjà)
  sqlQuery(loc,paste("drop table if exists dm_vecteurs.",tolower(i),sep=""))
  if(i=="DEPARTEMENTS"){i <- "DEPARTEMENT"}else{}
  system(paste("shp2pgsql -s 2154 -c -D -W \"latin1\" -I ",repBase,i,".shp ",schema,".",i," | psql -d sol_elevage -h localhost -U jb",sep=""))
  
  # Nettoyage
  system(paste("rm -d ",repBase,"*",sep=""))
}

# Construction du code canton pour les jointures avec les données agreste >> "code_canton"
sqlQuery(loc,paste("alter table ",schema,".canton
              add column code_canton varchar(5);
              UPDATE ",schema,".canton
              SET code_canton = code_dept || code_cant",sep=""))

```

```{r,highlight=TRUE,eval=FALSE}
# Intégration des petites régions agricoles

# Téléchargement
URL_pra <- "http://agreste.agriculture.gouv.fr/IMG/zip/comm-ra-pra2007.zip"
system(paste("wget -P ",repBase," ",URL_pra,sep=""))  

# Décompression
system(paste("7z e ",repBase,"comm-ra-pra2007.zip -o",repBase," -y",sep=""))

# Intégration dans la BDD (suppression si le vecteur exsite déjà)
pra <- read.table(file=paste(repBase,"comm-ra-pra2007.txt",sep=""), header=TRUE,sep='\t',fileEncoding="latin1",quote="Na",fill=TRUE)
pra <- pra[,1:10]

sqlQuery(loc,paste("drop table if exists public.pra",sep=""))
sqlSave(loc,pra,tablename="pra")

# Nettoyage
system(paste("rm -d ",repBase,"*.txt",sep=""))

# Jointure de la table pra vers la table commune (ici, on ajoute le code pra à la couche commune)
sqlQuery(loc,"alter table commune add column pra text")
sqlQuery(loc,"update commune
              set pra = petiterégionagricole from(
              select pra.petiterégionagricole, pra.codecommune
              from pra) as s1 where commune.INSEE_com=s1.codecommune")
```

# Les données sols

Présenter rapidement données sols inclusent dans la base
 
| Catégorie                             | Type de données                     | Source                         | Contacts                                     |
|---------------------------------------|-------------------------------------|--------------------------------|----------------------------------------------|
| BDAT                                  | ShapeFiles (L93)                    | InfoSol                        | infosol@inra.fr                              |
| LUCAS                                 | ShapeFiles (L93)                    | JRC                            |                                              |
| BDGSF                                 | ShapeFiles (L93)                    | JRC                            |                                              |
| Stock de carbone                      | Raster (L93)                        | Interne IDELE                  | infosol@inra.fr                              |
| Biomasse microbienne                  | Raster (L93)                        | IGN + Agreste                  | infosol@inra.fr                              |

## La BDAT

Deux types de données sont intégrées dans la base de données. Dans un premier temps, les données BDAT disponibles sur le XXX sont intégrées. Il s'agit de statistiques descriptives aggrégées à l'échelle du canton. Ces données ne sont pas exploitables dans le cadre d'une étude sur l'évolution temporelle car le nombre d'effectif n'est pas le même d'une année à l'autre. Le deuxième jeu de données intégrées prend en compte ce biais par un ré-échantillonnage artificiel des analyses afin d'assurer le calcul de statistiques descriptives basées sur un même nombre d'effectif.

### Importation des données de la BDAT non harmonisées 

Ces données sont uniquement utilisées pour les analyses pédologiques où les variations temporelles sont minimes. Pour le moment, seules les analyses sur la texture du sol sont exploitées (taux d'argile notamment).

```{r importBDAT, eval=FALSE}
period <- c("9094","9599","0004","0509")
# Selection des fichiers à importer
ListBDAT <- gsub("\\.txt$","",list.files(paste(repBDAT,"BDAT_datapaper/",sep=""),pattern="\\.txt$"))
#ListBDAT <- gsub("\\.txt$","",list.files(repBDAT,pattern="bdat_canton$"))#Voir pour sélectionner uniquement les fichiers bdat_canton

for(i in ListBDAT){
  print(i)
  # Lecture du fichier
  txtBDAT <- read.table(paste(repBDAT,i,".txt",sep=""),sep="\t",header=TRUE)
  txtBDAT[,2] <- as.character(txtBDAT[,2])
  
  # Intégration dans la bdd
  sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
  sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))
}

# Ajout des données texturales (médianes sur les années 1990-2009)



```
### Importation des données de la BDAT harmonisées

la liste d'instructions suivante intègre les analyses de la BDAT ré-échantillonnées dans la base de données.

```{r importBDAT_harmo, eval=FALSE}
# Selection des fichiers à importer
ListBDAT <- gsub("\\.csv$","",list.files(paste(repBDAT,"BDAT_harmonisee/",sep=""),pattern="\\.csv$"))

for(i in ListBDAT){
  print(i)
  # Lecture du fichier
  txtBDAT <- read.csv(paste(repBDAT,"BDAT_harmonisee/",i,".csv",sep=""),sep=";",header=TRUE,colClasses=c("canton"="character"))

    # Intégration dans la bdd
  sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
  sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))
}
```

## La BDGSF (en cours)

La base de données européenne des sols a été téléchargée sur ce [cite](http://eusoils.jrc.ec.europa.eu/ESDB_Archive/ESDB_data_1k_raster_intro/ESDB_1k_raster_data_intro.html). 
Le shapefiles semble avoir des soucis de topologie et son découpage pose qq problèmes avec gdal (ERROR 1: TopologyException: Input geom 1 is invalid: Self-intersection at or near point 199362.60000010004 6798705.2000011709 at 199362.60000010004 6798705.2000011709). Une solution serai d'importer la base européenne vers la base postgis, controler la topo et faire le découpage ensuite.

Contrôle de la topologie avec QGis ([vérificateur de la topologie](http://smsgis.co.za/using-qgis-fix-geometry-errors/))

```{r importBDGSF, eval=FALSE}
repbdgsf <- "/media/sf_GIS_ED/Dev/Data/Sol/bdgsf/"
vectorname <- "sgdbe4_0.shp"
file1 <- paste(repbdgsf,vectorname,sep="")
file2 <- paste(repbdgsf,"L93_",vectorname,sep="")
file3 <- paste(repbdgsf,"Fr_L93_",vectorname,sep="")
shaperef <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT.shp"

# Changement de la projection (L93) et masque pour importer uniquement les contours de la france métropolitaine
system(paste("ogr2ogr -t_srs EPSG:2154 ",file2," ",file1," -overwrite",sep=""))
#system(paste("ogr2ogr -clipsrc ",shaperef," ",file3," ",file2," -overwrite",sep=""))

# Intégration dans la base sous forme de postGIS
system(paste("shp2pgsql -s 2154 -c -D -W \"latin1\" -I ",file1," | psql -d sol_elevage -h localhost -U jb -s bdgdf",sep=""))#option -s ne fonctionnne pas

# Suppression des fichiers temporaires
system(paste("rm ",file1," ",file2,sep=""))
```

## Données élaborées LUCAS (en cours)

```{r eval=FALSE}
rep_rasterjrc <- "/media/sf_GIS_ED/Dev/Data/Sol/Lucas/Elaborees/"
list.rast <- list.files(rep_rasterjrc,pattern=".tif$")#Le $ signifie la fin du pattern
shaperef <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT.shp"
for(i in list.rast){
    # 
    system(paste("gdalwarp -t_srs EPSG:2154 ",rep_rasterjrc,i," ",rep_rasterjrc,"tmp",i," -overwrite",sep=""))
    system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef," -crop_to_cutline ",rep_rasterjrc,"tmp",i," ",rep_rasterjrc,"fr_",i," -overwrite",sep=""))
    system(paste("rm ",rep_rasterjrc,"tmp",i,sep=""))
    
    # Intégration dans la base postgis
    #system(paste("shp2pgsql -s 2154 -c -D -W \"latin1\" -I ",rep_rasterjrc,"sgdbe4_0.shp | psql -d sol_elevage -h localhost -U jb -s bdgdf",sep=""))#option -s ne fonctionnne pas
} 
```

## Les données LUCAS soil Database (en cours)

Ici, on présente l'intégration des différentes données lucas.
Il y a les données de bases lucas topsoil et également d'autres données élaborées en partie construite avec LucasTopsoil.


```{r importLucas_topsoil, eval=FALSE}
# Décompression de l'archive télécharger après une demande ici : http://eusoils.jrc.ec.europa.eu/content/lucas-2009-topsoil-data#tabs-0-description=1&tabs-0-description-2=
system("unzip Data/Lucas/LUCAS_TOPSOIL_v1.zip -d Data/Lucas/Topsoil/.")

# Conversion vers un fichier csv
installXLSXsupport()
Lucasdf <- read.xls(paste(repLucas,"Topsoil/LUCAS_TOPSOIL_v1.xlsx",sep=""),sheet="Sheet1")
Lucasdf <- Lucasdf[complete.cases(Lucasdf[,c("GPS_LAT","GPS_LONG")]),]
# Enregistrement dans la base locale
sqlSave(loc,Lucasdf)

# Conversion en postgis/définition du srid
sqlQuery(loc,"ALTER TABLE Lucasdf ADD COLUMN geom geometry(Point,4326);
              UPDATE Lucasdf SET geom = ST_SetSRID(ST_MakePoint(gps_long, gps_lat), 4326);")

# Reprojection en L93
sqlQuery(loc,"ALTER TABLE Lucasdf
              ALTER COLUMN geom 
              TYPE Geometry(Point, 2154) 
              USING ST_Transform(geom, 2154);")
 
# Sélection spatiale des points Lucas pour la France
sqlQuery(loc,"CREATE table public.test AS
              SELECT *
              FROM Lucasdf
              WHERE ST_Contains(Lucasdf.the_geom,commune.the_geom")

SELECT "KDhh_survey".* 
FROM
public."KDhh_survey",
public."ur_pilot_survey"
WHERE
ST_contains(public."ur_pilot_survey".the_geom, public."KDhh_survey".the_geom);



system("v.select --overwrite ainput=lucasref_L93 atype=point binput=franceL93 output=lucasref_L93_fr")

# Exportation vers un shapefile
system("v.out.ogr input=lucasref_L93_fr type=point dsn=/home/jb/Bureau/PrLucas/")

# Exportation vers PostGis
sqlQuery(databaracoa,"drop table if exists data.lucastopsoil_fr")
system("shp2pgsql -c -s 2154 /home/jb/Bureau/PrLucas/lucasref_L93_fr.shp data.lucastopsoil_fr| psql -h baracoa.orleans.inra.fr -p 5434 -U jbparoissien jbparoissien")

# Préparation finale
#Ajout d'une colonne geom pour le système de coodonnées lambert2etendue

# Reprojection de la table en lambert9
# Ajout des colonnes

sqlQuery(databaracoa,paste("select addgeometrycolumn('data','lucastopsoil_fr', 'the_geom_l2e',27582,'POINT',2)",sep=""))
sqlQuery(databaracoa,paste("select addgeometrycolumn('data','lucastopsoil_fr', 'the_geom_l93',2154,'POINT',2)",sep=""))
sqlQuery(databaracoa,paste("update lucastopsoil_fr set the_geom_l93 = geom",sep=""))
sqlQuery(databaracoa,paste("select DropGeometryColumn('data','lucastopsoil_fr','geom')",sep=""))

# Reprojection
sqlQuery(databaracoa,paste("update lucastopsoil_fr set the_geom_l2e = st_setsrid(st_transform(st_setsrid(the_geom_l93,2154),27582),27582);",sep=""))

# Pour connaître les coordonnées géographiques
sqlQuery(databaracoa,"alter table lucastopsoil_fr
	 add column x_l93 double precision;
	 alter table lucastopsoil_fr
	 add column y_l93 double precision;
	 alter table lucastopsoil_fr
	 add column x_l2e double precision;
 	 alter table lucastopsoil_fr
	 add column y_l2e double precision;")


sqlQuery(databaracoa,"update lucastopsoil_fr set x_l93 = ST_X(ST_SetSRID(the_geom_l93, 2154));
	 update lucastopsoil_fr set y_l93 = ST_Y(ST_SetSRID(the_geom_l93, 2154));")

sqlQuery(databaracoa,"update lucastopsoil_fr set x_l2e = ST_X(ST_SetSRID(the_geom_l2e, 27582));
	 update lucastopsoil_fr set y_l2e = ST_Y(ST_SetSRID(the_geom_l2e, 27582));")


```

```{r importLucas, eval=FALSE}
# Décompression des différentes archives vers le réperoire 'Elaborees'
LucasData <- c("AWC_EU23","BulkDensity_EU23","Clay_EU23","CoarseFragments_EU23","ER100mFinal","ESDB_soil_compaction_pack")
for(i in LucasData){
  system(paste("unzip ",repLucas,i,".zip -d ",repLucas,"Elaborees/.",sep=""))
}

```

# Les données d'occupation du sol

Les données d'occupation du sol proviennent du Recencement Agricole (RA) et de la base Corine Land Cover (CLC). Cette section présente l'intégration des données brutes dans la base de données.

## Corine Land Cover (CLC)

L'ensemble des feuilles excel du fichier `stats_clc_commune_niveau_2.xls` téléchargé à cette [adresse](http://www.statistiques.developpement-durable.gouv.fr/fileadmin/documents/Produits_editoriaux/Donnees_en_ligne/Territoires/clc-2012/stats-clc-2012-commune-niveau-2.zip) est introduit dans le schéma `clc` de la base de données. Les données représentent la surface des occupations du sol clc de niveau 2 par canton.

```{r importCLC,eval=FALSE}
# Intégration du fichier excel stats_clc_commune_niveau_2.xls
installXLSXsupport()
fichier <- paste(repCLC,"stats-clc-2012-commune-niveau-2/stats_clc_commune_niveau_2.xls",sep="")
feuilles <- c("CLC90","CHANGEMENTS90_00","CLC00_REVISEE","CHANGEMENTS00_06","CLC06_REVISEE","CLC12","CHANGEMENTS06_12")
schema <- "clc"
for(i in feuilles ){
  xlsfiles <- read.xls(fichier,sheet=i,header=TRUE,fileEncoding="latin1",sep=",")

  # Enregistrement dans la base locale
  sqlQuery(loc,paste("drop table if exists ",schema,".",i,sep=""))
  sqlSave(loc,xlsfiles,tablename = paste(schema,".",i,sep=""))
}
```

## Les données du recensement agricole (RA)

Les données du recensement agricole ont été extraites de requêtes lancées sur [Disard](http://cybergeo.revues.org/23155). L'ensemble des tables intégré dans la base de données est présenté dans le tableau ci-dessous.

```{r,eval=TRUE,warning=FALSE}
# Description des tables (voir pour rajouter les métadonnées dans les commentaires de table)
metatable <- read.csv(paste(repmetadonnees,"Catalogue_table.csv",sep=""),sep=";")
pander(metatable)
```
Dans les tables de sorties de Disard, le numéro de canton et le nom du chef lieu sont enregistrés dans le même champs. Dans les commandes suivantes, le numéro de canton et le nom du chef lieu sont dissociés dans un champs qui leur est propre pour faciliter les jointures futures.  

```{r importagreste,eval=TRUE,warning=FALSE,}
installXLSXsupport()
schema <- "ra" #Nom du schema de stockage des données
catalogue <- read.csv(paste(repmetadonnees,"Catalogue_table.csv",sep=""),sep=";")
Liste_fichiers <- catalogue[(catalogue[,1] %in% schema),2]
Commentaires <- catalogue[(catalogue[,1] %in% schema),3]
#Liste_fichiers <- gsub("\\.xls$","",list.files(repagreste,pattern="\\.xls$"))# Liste des xls présent dans le répertoire de travail (repagreste)

cpt <- 0
for(i in Liste_fichiers){
  if(i=="UGBTA_canton880010"){
  # Intégration spéciale pour la table UGBTA_canton880010.csv" (Téléchargé http://agreste.agriculture.gouv.fr/IMG/xls/Donnees_principales__canton_departement_.xls)
  tablecsv <- read.csv(paste(repagreste,"UGBTA_canton880010.csv",sep=""),header=TRUE,sep=";",colClasses=c("num_canton"="character"))
  
  # Configuration des types de champs
  tablecsv[,-1] <- data.frame(lapply(tablecsv[,-1], function(v) {
    as.numeric(as.character(v))}))

  # Suppression des lignes avec une SAU=0
  tablecsv <- tablecsv[tablecsv$SAU2000>0 & tablecsv$SAU2010>0 & tablecsv$SAU1988>0,]

  # Suppression du canton 5496 (valeur abérante)
  tablecsv <- tablecsv[tablecsv$num_canton != "5496",]

  sqlQuery(loc,paste("drop table if exists ",schema,".UGBTA_canton880010",sep=""))
  sqlSave(loc,tablecsv,tablename = paste(schema,".UGBTA_canton880010",sep=""))
  
  # Ajout d'un commentaire
  print(sqlQuery(loc,paste("
	    COMMENT ON TABLE ",schema,".",i," IS \'",Comment,".\';",sep="")))
  next
  }else{}
  
  cpt <- cpt + 1 
  # Lecture du fichier
  xlsfiles <- read.xls(paste(repagreste,i,".xls",sep=""),sheet="Feuille1",header=TRUE,fileEncoding="latin1",sep=",")  
  Comment <- Commentaires[cpt]

  # Configuration des types de champs
  xlsfiles[,-1] <- data.frame(lapply(xlsfiles[,-1], function(v) {
    as.numeric(as.character(v))}))
  
  # Extraction du code canton  
  toto <- as.character(xlsfiles[[1]])
  numcanton <- regmatches(toto,gregexpr('[0-9]+.[0-9]+',toto))
  xlsfiles["Num_canton"] <- as.character(unlist(numcanton))
  
  # Extraction du nom du chef lieu (tout ce qu'il y a après le tiret)
  #regmatches(popo2,gregexpr('^[a-zA-Z]+$',popo2))
  xlsfiles["nom_chflieu"] <- gsub2(".*- ", "", as.character(unlist(toto)))
   
  # Enregistrement dans la base locale
  sqlQuery(loc,paste("drop table if exists ",schema,".",i,sep=""))
  sqlSave(loc,xlsfiles,tablename = paste(schema,".",i,sep=""))
  
  # Ajout d'un commentaire
  print(sqlQuery(loc,paste("
	    COMMENT ON TABLE ",schema,".",i," IS \'",Comment,".\';",sep="")))
}
```

## Les grandes régions d'élevage

Les grandes régions d'élevage représentent un zonage basé sur l'utilisation du sol (part de la SFP/des cultures et dans la SFP part de la STH, des PT et du maïs, et recroisement avec les zones à contraintes naturelles (montagne et zones sèches)). Le zonage a évolué depuis 1995, avec des correctifs dans certaines zones (Limousin notamment) et surtout un redécoupage en fonction de l’orientation des productions d’élevage, notamment bovin (lait ou viande) et du contexte pédoclimatique (séparer la Bretagne et les piémonts, les Bassin parisien et aquitain par exemple).

Le zonage intégré dans la base a été fourni par Christophe Perrot. Pour plus d'information sur la construction, contacter l'auteur (rajouter adresse)
Pour faciliter la distinction des différents niveaux de régions, une nomenclature a été créée. Elle est consultable [ici](https://github.com/Rosalien/GISEDSol/tree/master/Documentation/Metadonnees/Nomenclature_regionelevage.csv)

```{r importRegionElevage,eval=FALSE}
repRegionElevage <- "/media/sf_GIS_ED/Dev/Data/Regions_elevages/"
regelevage <- read.csv(paste(repRegionElevage,"zonage_idele_13_modif.csv",sep=""),header=TRUE,sep=";")  
schema <- "public" 

# Voir pour supprimer les valeurs avec "." (les mettres en NA)
sqlQuery(loc,paste("drop table if exists ",schema,".regelevage",sep=""))
sqlSave(loc,regelevage,tablename=paste(schema,".regelevage",sep=""))

# Jointure vers la table dm_vecteurs.commune
table_dm <- "dm_vecteurs.commune"
var <- "regelevage"

for(i in c("zonage_simple","zonage_cplt")){
  
  sqlQuery(loc,paste("alter table ",table_dm,"
                      drop column if exists ",i,sep=""))
  
  sqlQuery(loc,paste("alter table ",table_dm,"
                      add column ",i," text;
                      update ",table_dm,"
                      SET ",i," = s1.",i," from(
                      select ",i,",codecommune
                      from ",schema,".regelevage) as s1
                      where ",table_dm,".insee_com=s1.codecommune::text",sep=""))
  
  # Ajout d'un commentaire sur la nouvelle colonne créée
  print(sqlQuery(loc,paste("
  COMMENT ON COLUMN ",table_dm,".",i," IS \'",i," des principales régions d élevage. La table ",schema,".regelevage présente la signification des codes utilisés pour le zonage. Source DG AGRI RICA UE 2012 - traitement IDELE\';",sep="")))
}

# Agrégation à l'échelle du canton (valeur majoritaire)
table_dm <- "dm_vecteurs.canton"
var <- "regelevage"

for(i in c("zonage_simple","zonage_cplt")){

  # Suppression de la colonne si déjà existante
  sqlQuery(loc,paste("alter table ",table_dm,"
                    drop column if exists ",i,sep=""))
  
  # Création de la colonne, aggrégation par canton (valeur majoritaire, fonction mode()) et jointure vers la table canton 
  sqlQuery(loc,paste("alter table ",table_dm,"
                    add column ",i," text;
                    update ",table_dm,"
                    SET ",i," = s1.",i," from(
                    select (code_dept || code_cant) as num_canton,mode() within group (order by regelevage.",i,") as ",i,"
                    from ",schema,".regelevage                  
                    inner join dm_vecteurs.commune as c on c.insee_com=regelevage.codecommune
                    group by code_dept || code_cant) as s1
                    where ",table_dm,".code_canton=s1.num_canton::text",sep=""))

  #Ajout d'un commentaire sur la nouvelle colonne créée
 print(sqlQuery(loc,paste("
  COMMENT ON COLUMN ",table_dm,".",i," IS \'",i," des principales régions d élevage par canton. La valeur est issue des données communales (",schema,".regelevage) et représente la valeur majoritaire par canton. La table ",schema,".regelevage présente la signification des codes utilisés pour le zonage. Source DG AGRI RICA UE 2012 - traitement IDELE\';",sep="")))
}
```

# Les données climatiques

Plusieurs sources de données climatiques sont disponibles. Pour le moment, seul le travail de [Joly et al., 2010](http://cybergeo.revues.org/23155) est exploité. Les autres sources de données pourront être intégrées ultérieurement dans la base de données, dans le schéma `climato`.

## Topologie du climat en France

Les données climatiques utilisées proviennnent du travail de [Joly et al., 2010](http://cybergeo.revues.org/23155) sur la typologie du climat en France. Les commandes suivantes importent la table `table_commun.txt` disponible à cette [adresse](Rajouter adresse). Les données climatiques sont ensuite aggrégées par canton dans le fichier de suivie [Traitement_climato.Rmd](https://github.com/Rosalien/GISEDSol/blob/master/Fichiers_suivis/Traitements/Suivis/Traitements_climato.Rmd). 

```{r importClimatL,eval=FALSE}
##Paramètres
rep_climato <- paste(repdata,"Climato/Joly2010",sep="") #Répertoire des données climatiques à intégrer

#Lecture de la table
jolytable <- read.csv(paste(rep_climato,"/tableau_commun.txt",sep=""),sep="\t",header=TRUE,colClasses=c("DC"="character"),na.strings = c("-9999","2999.9"))#Dans la lecture, on spécifie le type de colonne pour DC (text) afin d'assurer les jointures. Les valeurs -9999 sont considérées comme NA. Les valeurs 2999.9 sont a priori des erreurs (valeurs abérantes)
jolytable$HPLUIE_AN <- as.numeric(as.character(jolytable$HPLUIE_AN))

# Les valeurs 9 et 0 pour la colonne typo_clim sont considérées comme NA
jolytable$TYPO_CLIM[jolytable$TYPO_CLIM==9 | jolytable$TYPO_CLIM==0] <- NA

# Correction de la valeur abérante en hauteur de pluie annuelle sur le canton 5814. Le problème vient d'une valeur extrème sur la commune 58125. La valeur est remplacée par la valeur de la commune voisine
jolytable[jolytable$DC %in% "58125","HPLUIE_AN"] <- jolytable[jolytable$DC %in% "58185","HPLUIE_AN"]

#Enregistrement vers la base, schéma climatot
sqlQuery(loc,"drop table if exists climat.climatjoly")
sqlSave(loc,jolytable,tablename="climat.climatjoly")

#Ajout d'un commentaire pour décrire la table
sqlQuery(loc,"COMMENT ON TABLE climat.climatjoly IS 'Table importée à cette adresse : http://cybergeo.revues.org/26894?file=1. Le fichier initial est au format TXT, comporte 36267 lignes (une par commune) et 16 colonnes, soit respectivement, l’identifiant communal INSEE (DC) les 14 variables climatiques traitées et une variable définissant le type climatique où se range la commune. La valeur codant l’absence de données est -9999. Plus infos : http://cybergeo.revues.org/23155 
L’usage de ces fichiers est strictement limité au domaine des services publics. Toute exploitation de la base climatique, totale ou partielle (cartes, tableaux, etc.), doit être accompagnée de la mention de la source décrite ainsi :
Source : Base de données climatiques communales 2009. THEMA Université de Franche-Comté, CNRS UMR6049 (F-25000 Besançon) /CESAER INRA UMR1041 (F-21000 Dijon) ; d’après Météo France 1971-2000.';")
```

# La topographie

## Données SRTM

Le téléchargement des données du SRTM a été réalisé sur [earthexplorer](https://earthexplorer.usgs.gov/) de la manière suivante :

- Inscription,
- Sélection la zone géographique (search criteria >> Use Map). **Attention,** le nombre de tuile est limité à 100. Bien sélectionner la zone désirée ou le faire en deux fois.
- Sélection du type de données souhaité (Data Sets >> Digital Elevation >> SRTM >> SRTM 1 Arc-Second Global),
- Cliquer sur 'Results' et sélectionner l'ensemble des tuiles à télécharger (add to bulk download >> submit standing request),
- Utilisation du logiciel [BDT](https://earthexplorer.usgs.gov/bulk) pour télécharger les tuiles commandées. **Attention**, il est nécessaire d'ouvrir les ports 4448 pour utiliser le logiciel.


```{r,eval=FALSE}
# Décompression de tous les fichiers zip présent dans le répertoire repSRTM
repSRTM <- "/media/sf_GIS_ED/Dev/Data/Topographie/SRTM/SRTM_1_Arc_Second_Global/"
#system(paste("unzip ",repSRTM,"'*.zip' -d ",repSRTM,sep=""))

shaperef3035 <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT.shp"
vrtname <- paste(repSRTM,"mergebil.vrt",sep="")
file1tmp <- paste(repSRTM,"tmp1.tif",sep="")
file2tmp <- paste(repSRTM,"tmp2.tif",sep="")
srtmName <- paste(repSRTM,"Fr_L93_srtm.tif",sep="") #Nom du raster final

# Liste des tuiles à assembler
listbil <- paste(repSRTM,list.files(repSRTM,pattern="\\.bil$"),sep="",collapse=" ")#gsub("\\.bil$","", list.files(repSRTM,pattern="\\.bil$"))

# Création d'un raster virtuel pour assembler les rasters (plus rapide que gdal_merge.py)
system(paste("gdalbuildvrt ",vrtname," ",listbil,sep=""))

# Conversion au format tif
system(paste("gdal_translate -co TILED=YES -co 'COMPRESS=LZW' -co BIGTIFF=YES -a_srs EPSG:4326 ",vrtname," ",file1tmp,sep=""))#testvrt.tif

# Reprojection
system(paste("gdalwarp -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",file1tmp, " ",file2tmp," -overwrite",sep=""))

# Découpage 
system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef2154," -crop_to_cutline -co 'COMPRESS=LZW' ",file2tmp," ",srtmName," -overwrite",sep=""))

# Création de pyramides pour faciliter la visualisation
systemp(paste("gdaladdo -r average ",srtmName," 2 4 8 16",sep=""))

# Suppression des fichiers temporaires
system(paste("rm ",file1tmp," ",file2tmp,sep=""))
```

Voir la pertinence de mettre le raster sur PostGIs (et penser aussi à la conversion vers spatialite)

## Données européennes

Le modèle numérique de terrain provient du programme Européen Copernic. Les données ont été téléchargées à cette [adresse](http://www.eea.europa.eu/data-and-maps/data/eu-dem). Pour l'exploitation des données, le fichier au format GeoTIFF est découpé aux contours de la France et projeté en Lambert 93 (EPSG 2154). Le raster est ensuite intégré dans le serveur postgis. Ces procédures sont réalisées avec la fonction `gdalwarp` et XXX dans les listes d'instructions ci-dessous. Les statistiques calculées avec ces données sont réalisées dans le fichier de suivi 
[FS_bdd_elab_topo.Rmd](https://github.com/Rosalien/GISEDSol/blob/master/Fichiers_suivis/BDD/Suivis/FS_bdd_elab_topo.Rmd).

Les tuiles téléchargées sont les suivantes : E30N20; E40N20;E30N30 (Pour connaître les tuiles, voir les webservices à cette adresse :http://image.discomap.eea.europa.eu/arcgis/services/Elevation/DEM_v_1_1/MapServer/WMSServer?version=1.3.0 ).
Les instructions suivantes permettent d'assembler les 3 rasters, de les découper aux contours de la France et de projeter le dernier fichier en Lambert 93 (epsg:2154).


```{r,eval=FALSE}
reptopo <- "/media/sf_GIS_ED/Dev/Data/Topographie/dem_copernicus/" #répertoire contenant les rasters à assembler
shaperef3035 <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT_3035.shp" #Shapefiles utilisé pour le découpage aux contours de la France
listtif <- paste(reptopo,c("eu_dem_v11_E40N20.tif","eu_dem_v11_E30N30.tif","eu_dem_v11_E30N20.tif"),sep="",collapse=" ")
vrtname <- "europ_dem.vrt"
file1tmp <- paste(reptopo,"tmp1.tif",sep="")
file2tmp <- paste(reptopo,"tmp2.tif",sep="")
Fr_demName <- paste(reptopo,"Fr_L93_eudem.tif",sep="")
Fr_demNamelight <- paste(reptopo,"Fr_L93_90eudem.tif",sep="")

# Création d'un raster virtuel pour assembler les rasters (plus rapide que gdal_merge.py)
system(paste("gdalbuildvrt ",reptopo,vrtname," ",listtif,sep=""))

# Conversion au format tiff
system(paste("gdal_translate -co TILED=YES -co 'COMPRESS=LZW' -co BIGTIFF=YES -a_srs EPSG:3035 ",reptopo,vrtname," ",file1tmp,sep=""))#testvrt.tif

# Découpage aux contours de la France
system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef3035," -crop_to_cutline -co TILED=YES -co BIGTIFF=YES -co 'COMPRESS=LZW' ",file1tmp," ",file2tmp," -overwrite",sep=""))

# Reprojection en Lambert 93
system(paste("gdalwarp -co BIGTIFF=YES -co TILED=YES -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",file2tmp," ",Fr_demName," -overwrite",sep=""))

# Création de pyramides pour faciliter la visualisation
systemp(paste("gdaladdo -r average ",Fr_demName," 2 4 8 16",sep=""))

# Nettoyage des fichiers temporaires
system(paste("rm ",file1tmp," ",file2tmp,sep=""))

# Création d'un raster plus léger pour les analyses
system(paste("gdalwarp -tr 90 90 -co BIGTIFF=YES -co TILED=YES -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",Fr_demName," ",Fr_demNamelight," -overwrite",sep=""))

```

Le raster est ensuite intégré sur le serveur PostGIS (voir la pertinence). Si celà ne fonctionne pas, revoir les scripts pour calculer les moyennes entre vecteurs et rasters avec R.

```{r demtopostgis,eval=FALSE}
rastername <- paste(reptopo,"Fr_L93_",rastername,sep="")
rpostgisname <- "m_rasters.dem_copernicus"

system(paste("raster2sql -s 2154 -t 30x30 -f -I -Y myRaster.tif ",rpostgisname," > raster.sql`
psql -h localhost -U jb -d sol_elevage -f raster.sql",sep=""))
```

# Les métadonnées

## Création de métadonnées pour les schémas

La description des schémas de la base de données est stockés dans le fichier [Catalogue_schema.csv](https://github.com/Rosalien/GISEDSol/tree/master/Documentation/Metadonnees/Catalogue_schema.csv) et ces commentaires sont intégrés dans la base de données avec la commande suivante :

```{r,eval=FALSE}
meta_schema <- read.csv(paste(masterrep,"Scripts/master/Documentation/Metadonnees/Catalogue_schema.csv",sep=""),sep=";",header=TRUE)
for(i in 1:nrow(meta_schema)){
  sqlQuery(loc,paste("COMMENT ON SCHEMA ",meta_schema[i,1]," IS '",meta_schema[i,2],"'",sep=""))
}
```

## Création d'une vue pour les métadonnées

Pour faciliter la compréhension des données stockées dans la base de données, une vue des métadonnées est générée à travers la commande suivante. Celle-ci est accessible sur `public.metadata`

```{r,eval=FALSE}
# Lancement du script sql pour créer une vue sur les métadonnées (accessible sur public.metadata)
system(paste("sh ",repfonctions,"bash/create_view_metada.sh",sep=""))
```


# Export de la base

Deux formats sont proposés pour exploiter la base de données :

- une base SQLite/Spatialite sous forme de fichier `.sqlite`,
- une sauvegarde de la base PostgreSQL/PostGIS sous forme de fichier texte (`sql`).

## Export vers SQLite/Spatialite

L'export de la base sous forme SQLite/Spatialite offre l'opportunité d'exploiter la base rapidement à travers l'ouverture du fichier directement connectable avec des clients de données géographiques (comme QGIS) ou de base de données (sqlitebrower). Lors de la conversion, l'arborescence des tables n'est pas maintenue et les commentaires des champs faisant office de métadonnées ont disparu.
Le fichier est néanmoins utile pour visualiser rapidement les données géographiques.
Rajouter lien vers des modes opératoires pour lire les données vers qgis.

```{r,eval=FALSE}
# Vérification (selon http://gis.stackexchange.com/questions/168819/fixing-ogr2ogr-without-spatialite-support)
ogrinfo --formats sqlite | grep 'spatialite' -i
ogrinfo --format sqlite | grep 'spatialite' -i
databasename <- "sol_elevage"

system(paste("ogr2ogr --config PG_LIST_ALL_TABLES YES --config PG_SKIP_VIEWS YES -f \"SQLite\" ",databasename,".sqlite -progress PG:\"dbname='",databasename,"' \
host='localhost' port='5432' user='jb' password='170284'\" -lco LAUNDER=yes \
  -dsco SPATIALITE=yes -lco SPATIAL_INDEX=yes -gt 65536",sep=""))
```

## Export de la base en PostgreSQL/PostGIS

La base de données est sauvegardée au format PostgreSQL/PostGIS avec la commande `pg_dump`. Cette commande permet de garder l'ensemble de la base de données mais pour être restaurer, l'utilisateur doit créer un serveur local Postgresql/PostGIS en suivant la procédure présentée dans le mode opératoire...
Les commandes suivantes présentent les instructions données pour sauvegarder la base de données.

```{r,eval=FALSE}
#pd_dump sol_elevage > test

pg_dump -Ft -b mabase > base.tar
pg_restore -d nouvellebase base.tar

pg_dump --inserts db_name -t table_name > pg_dump.sql

# http://sqlite.com/cvstrac/wiki?p=ConverterTools

#Export the data from your database as tab delimited text. Create one text file for every table in your database:
$ pg_dump -a <dbname> -t <tablename> > /tmp/<dumpfile> ...and so on...

#Trim off crap from header and footer from each file, eg:

$ nano -w /tmp/<dumpfile>

#If required, create SQLite file and tables corresponding to tables in the original PostgreSQL database:
$ sqlite3 <dbname> sqlite> CREATE TABLE ...etc...

#Finally, import the dump files into SQLite, remembering to change the default import separator to a <tab>:
$ sqlite3 <dbname> sqlite> .separator "\t" sqlite> .import <dumpfile> <tablename> 
```
