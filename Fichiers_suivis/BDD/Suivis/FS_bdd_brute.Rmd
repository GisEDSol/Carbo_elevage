---
title: "Fichier de suivi de la création de la base de données"
author: "Jean-Baptiste Paroissien"
date: "22/05/2017"
output:
  html_document:
    toc: yes
    toc_float: yes
    fig_caption: yes
    highlight: zenburn
    number_sections: yes
    theme: spacelab   
---

```{r setup, include=FALSE}
# Importation des paramètres de travail
source("/media/sf_GIS_ED/Dev/Scripts/master/Fonctions/R/importparametres.R")
repmaster <- "/media/sf_GIS_ED/Dev/Scripts/master/"
repdata <- "/media/sf_GIS_ED/Dev/Data/"
importparametres(repmaster=repmaster,repdata=repdata,dsn="PG:dbname='sol_elevage' host='localhost' port='5432' user='jb'")
```

# Objectifs

L'objectif de ce document est de centraliser dans un même fichier la création d'une base de données pour étudier l'impact de l'élevage sur la qualité des sols. Cette base contient plusieurs types de données en lien avec les données pédologiques, les données associées à l'occupation du sol et les données environnementales générales telles que le climat ou la topographie.

# Documentation

La documentation de cette base de données s'articule autour d'un document général consultable à cette [adresse](https://rawgit.com/GisEDSol/Carbo_elevage/master/Documentation/Modes_operatoires/MO_bdd.html).

# Création de la base

La création de la base postgresql/postgis est réalisée selon la procédure suivante :

```{r, engine='bash',highlight=TRUE,eval=FALSE}
sudo -i -u postgres
postgres=# CREATE USER jb;
postgres-# ALTER ROLE jb WITH CREATEDB;
postgres=# CREATE DATABASE SOL_ELEVAGE OWNER jb;
postgres-# ALTER USER jb WITH ENCRYPTED PASSWORD '******';
postgres=# CREATE DATABASE sol_elevage OWNER jb;
postgres=# alter user jb with superuser;
\q
exit
psql sol_elevage
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
```    
# Les données de bases

Les données dites de "bases" concernent les données administratives utilisées pour agréger certaines variables environnementales. Dans ce travail, les contours des communes, des cantons et des petites régions agricoles sont les principales échelles de travail utilisées. Elles ont été récupérées de la manière suivante :

- Communes et Canton : La version de 2011 de base de données [Geofla](http://professionnels.ign.fr/geofla) est utilisée. Cette version a été choisie pour être en accord avec les données du dernier recencement agricole (2010)
- Petite région agricole : Les données associées aux petites régions agricoles proviennent de [l'agreste](http://agreste.agriculture.gouv.fr/IMG/zip/comm-ra-pra2007.zip) et sont jointes aux données Geofla.

```{r,highlight=TRUE,eval=FALSE}

schema <- "dm_vecteurs"#nom du schéma pour intégrer les vecteurs

# Intégration des données communales et cantonnales dans la base
for(i in c("DEPARTEMENTS","CANTON","COMMUNE")){
  # Téléchargement
  URL <- paste("https://wxs-telechargement.ign.fr/oikr5jryiph0iwhw36053ptm/telechargement/inspire/GEOFLA_THEME-",i,"_2011_GEOFLA_1-1_SHP_LAMB93_FR-ED111/file/GEOFLA_1-1_SHP_LAMB93_FR-ED111.7z",sep="")
  system(paste("wget -P ",repBase," ",URL,sep=""))  

    # Décompression
  system(paste("7z e ",repBase,"GEOFLA_1-1_SHP_LAMB93_FR-ED111.7z -o",repBase," -y",sep=""))
  
  # Intégration dans la BDD (suppression si le vecteur exsite déjà)
  sqlQuery(loc,paste("drop table if exists dm_vecteurs.",tolower(i),sep=""))
  if(i=="DEPARTEMENTS"){i <- "DEPARTEMENT"}else{}
  system(paste("shp2pgsql -s 2154 -c -D -W \"latin1\" -I ",repBase,i,".shp ",schema,".",i," | psql -d sol_elevage -h localhost -U jb",sep=""))
  
  # Nettoyage
  system(paste("rm -d ",repBase,"*",sep=""))
}

# Construction du code canton pour les jointures avec les données agreste >> "code_canton"
sqlQuery(loc,paste("alter table ",schema,".canton
              add column code_canton varchar(5);
              UPDATE ",schema,".canton
              SET code_canton = code_dept || code_cant",sep=""))

# Création des contours de la France
sqlQuery(loc,"alter table dm_vecteurs.commune;
              add column pays text;
              update dm_vecteurs.commune
              set pays = 'fr';")

sqlQuery(loc,"DROP TABLE IF EXISTS dm_vecteurs.france")
sqlQuery(loc,"create table dm_vecteurs.france as
              SELECT ST_Multi(ST_Union(geom)) as geom
              FROM dm_vecteurs.commune As f
              GROUP BY pays")
```

```{r,highlight=TRUE,eval=FALSE}
# Intégration des petites régions agricoles

# Téléchargement
URL_pra <- "http://agreste.agriculture.gouv.fr/IMG/zip/comm-ra-pra2007.zip"
system(paste("wget -P ",repBase," ",URL_pra,sep=""))  

# Décompression
system(paste("7z e ",repBase,"comm-ra-pra2007.zip -o",repBase," -y",sep=""))

# Intégration dans la BDD (suppression si le vecteur existe déjà)
pra <- read.table(file=paste(repBase,"comm-ra-pra2007.txt",sep=""), header=TRUE,sep="\t",fileEncoding="latin1",quote="",fill=TRUE)
pra <- pra[,1:10]

sqlQuery(loc,paste("drop table if exists public.pra",sep=""))
sqlSave(loc,pra,tablename="pra")

# Nettoyage
system(paste("rm -d ",repBase,"*.txt",sep=""))

# Jointure de la table pra vers la table commune (ici, on ajoute le code pra à la couche commune)
sqlQuery(loc,"alter table commune add column pra text")
sqlQuery(loc,"update dm_vecteur.commune
              set pra = petiterégionagricole from(
              select pra.petiterégionagricole, pra.codecommune
              from pra) as s1 where commune.INSEE_com=s1.codecommune")

table_dm <- "dm_vecteurs.canton"
variablepra <- c("ra","pra")
nompra <- c("régionagricole","petiterégionagricole")
cpt <- 0
for(i in variablepra){
  cpt <- cpt + 1
  sqlQuery(loc,paste("alter table ",table_dm,"
                      drop column if exists ",i,sep=""))
  
  sqlQuery(loc,paste("alter table ",table_dm,"
                      add column ",i," varchar(6);
                      update ",table_dm,"
                      set ",i,"= s1.",nompra[cpt]," from(
                      select ",nompra[cpt],",(code_dept || code_cant) as num_canton
                      from dm_vecteurs.commune as c
                      inner join public.pra as pra on pra.codecommune=c.INSEE_com) as s1
                      where ",table_dm,".code_canton=s1.num_canton::text",sep=""))

  # Ajout d'un commentaire sur la nouvelle colonne créée
  print(sqlQuery(loc,paste("
  COMMENT ON COLUMN ",table_dm,".",i," IS \'",nompra[cpt]," du canton.\';",sep="")))
}

```

# Les données sols

## La BDAT

Deux types de données sont intégrés dans la base de données. Dans un premier temps, les données BDAT disponibles sur le site du [GISSol](http://www.gissol.fr/wp-content/uploads/2015/04/bdat.zip) sont intégrées. Il s'agit de statistiques descriptives agrégées à l'échelle du canton. Ces données ne sont pas exploitables dans le cadre d'une étude sur l'évolution temporelle car le nombre d'effectif n'est pas le même d'une année à l'autre. Ce jeu de données est utilisé pour l'analyse de variable "stable" dans le temps, comme la texture du sol par exemple.

Le deuxième jeu de données intégré prend en compte ce biais par un ré-échantillonnage artificiel des analyses afin d'assurer le calcul de statistiques descriptives basées sur un même nombre d'effectif. Ces données sont exploitées pour analyser l'évolution des teneurs en carbone organique dans les sols. 

### Importation des données de la BDAT non ré-échantillonnées

Ces données sont uniquement utilisées pour les analyses pédologiques où les variations temporelles sont minimes. Pour le moment, seules les analyses sur la texture du sol sont exploitées (taux d'argile notamment). Les données ont été téléchargées à cette [adresse](http://www.gissol.fr/wp-content/uploads/2015/04/bdat.zip).

```{r importBDAT, eval=FALSE}
period <- c("9094","9599","0004","0509")
# Selection des fichiers à importer
ListBDAT <- gsub("\\.txt$","",list.files(paste(repBDAT,"BDAT_datapaper/",sep=""),pattern="\\.txt$"))
#ListBDAT <- gsub("\\.txt$","",list.files(repBDAT,pattern="bdat_canton$"))#Voir pour sélectionner uniquement les fichiers bdat_canton

for(i in ListBDAT){
  print(i)
  # Lecture du fichier
  txtBDAT <- read.table(paste(repBDAT,i,".txt",sep=""),sep="\t",header=TRUE)
  txtBDAT[,2] <- as.character(txtBDAT[,2])
  
  # Intégration dans la bdd
  sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
  sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))
}

# Ajout des données texturales (médianes sur les années 1990-2009)
ListBDAT <- c("bdat_pra_9009","bdat_canton_9009")

for(i in ListBDAT){
  print(i)
  # Lecture du fichier
  txtBDAT <- read.table(paste(repBDAT,"BDAT_datapaper/",i,".txt",sep=""),sep="\t",header=TRUE)
  txtBDAT[,2] <- as.character(txtBDAT[,2])
  
  # Intégration dans la bdd
  sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
  sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))
}

```
### Importation des données de la BDAT ré-échantillonnées

La liste d'instructions suivante intègre les analyses des teneurs de la BDAT ré-échantillonnées dans la base de données.

#### Médianes

```{r importBDAT_harmomethodes, eval=FALSE}
nomdiff <- read.csv(paste(repmetadonnees,"Nomenclature_evolutionbdat.csv",sep=""),sep=",",colClasses = "character")

ListBDAT <- gsub("\\.csv$","",list.files(paste(repBDAT,"BDAT_harmonisee/mediane2/",sep=""),pattern="\\.csv$"))

for(i in ListBDAT){
    print(i)
    # Extraction de la valeur numérique
    comp <- unlist(regmatches(i,gregexpr('[0-9]+',i)))
    
    comp1 <- nomdiff[nomdiff$Code %in% comp,"Periode1"]
    comp2 <- nomdiff[nomdiff$Code %in% comp,"Periode2"]

    # Lecture du fichier
    txtBDAT <- read.csv(paste(repBDAT,"BDAT_harmonisee/mediane2/",i,".csv",sep=""),sep=";",header=TRUE,colClasses=c("Canton"="character"),encoding = "latin1")

    # Intégration dans la bdd

    sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
    sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))

     # Ajout d'un commentaire
     Comment <- paste("Statistiques de la médiane des médianes des teneurs en CO (corgox) des analyses de la BDAT ré-échantillonnées. Différence entre la période ",comp1," et la période ",comp2,sep="")

    print(sqlQuery(loc,paste("
      COMMENT ON TABLE bdat.",i," IS \'",Comment,".\';",sep="")))
  }
```

### 3ème quartile

```{r importBDAT_harmomethodes, eval=FALSE}
nomdiff <- read.csv(paste(repmetadonnees,"Nomenclature_evolutionbdat.csv",sep=""),sep=",",colClasses = "character")

ListBDAT <- gsub("\\.csv$","",list.files(paste(repBDAT,"BDAT_harmonisee/dquartile2/",sep=""),pattern="\\.csv$"))

  for(i in ListBDAT){
    print(i)
    # Extraction de la valeur numérique
    comp <- unlist(regmatches(i,gregexpr('[0-9]+',i)))
    
    comp1 <- nomdiff[nomdiff$Code %in% comp,"Periode1"]
    comp2 <- nomdiff[nomdiff$Code %in% comp,"Periode2"]

    # Lecture du fichier
    txtBDAT <- read.csv(paste(repBDAT,"BDAT_harmonisee/dquartile2/",i,".csv",sep=""),sep=";",header=TRUE,colClasses=c("Canton"="character"),encoding = "latin1")

    # Intégration dans la bdd

    sqlQuery(loc,paste("drop table if exists bdat.",i,sep=""))
    sqlSave(loc,txtBDAT,tablename=paste("bdat.",i,sep=""))

     # Ajout d'un commentaire
     Comment <- paste("Statistiques de la médiane du troisième quartile des teneurs en CO (corgox) des analyses de la BDAT ré-échantillonnées. Différence entre la période ",comp1," et la période ",comp2,sep="")

    print(sqlQuery(loc,paste("
      COMMENT ON TABLE bdat.",i," IS \'",Comment,".\';",sep="")))
  }
```

## Les données de l'European Soil Data Centre (ESDAC)


### La BDGSF 

La base de données européenne des sols a été téléchargée sur ce [site](http://esdac.jrc.ec.europa.eu/content/european-soil-database-v20-vector-and-attribute-data#tabs-0-description=1&tabs-0-description-2=). La base est à l'échelle européenne et l'objectif est de découper la base au contour de la France. Au final, cette base correspondra globalement à la Base de données Géographique des Sols de France (BDGSF) produite par InfoSol.

Les instructions ci-dessous permettent de :

- changer la projection du shapefiles vers le Lambert93 (avec ogr2ogr),
- intégrer le shapefiles dans la base de données,
- découper le shapefiles au contour de la France.

Les traitements associés à ces données sont réalisés dans le fichier [FS_bdd_elab_bdgsf.Rmd](https://github.com/GisEDSol/Carbo_elevage/blob/master/Fichiers_suivis/BDD/Suivis/FS_bdd_elab_bdgsf.Rmd).

```{r importBDGSF, eval=FALSE}
repbdgsf <- "/media/sf_GIS_ED/Dev/Data/Sol/ESDAC/bdgsf/"
vectorname <- "SGDB_PTR.shp"
file1 <- paste(repbdgsf,vectorname,sep="")
file2 <- paste(repbdgsf,"L93_",vectorname,sep="")
file3 <- paste(repbdgsf,"Fr_L93_",vectorname,sep="")
schema <- "esdac"
sortie_eu <- "bdgsf_eu"
sortie_fr <- "bdgsf"

# Changement de la projection (L93) 
system(paste("ogr2ogr -t_srs EPSG:2154 ",file2," ",file1," -overwrite",sep=""))

# Intégration dans la base sous forme de postGIS
sqlQuery(loc,paste("drop table if exists ",schema,".",sortie_eu,sep=""))
system(paste("shp2pgsql -s 2154 -c -D -W \"latin1\" -I ",file2," ",schema,".",sortie_eu,"| psql -d sol_elevage -h localhost -U jb",sep=""))

bdgsf <- sqlQuery(loc,paste("select * from ",schema,".",sortie_eu," limit 1",sep=""))
n_bdgsf <- names(bdgsf)[-length(bdgsf)]
n_bdgsf <- paste(n_bdgsf,collapse=",")

# Intersection avec postgis
sqlQuery(loc,paste("DROP TABLE IF EXISTS ",schema,".",sortie_fr,sep=""))
sqlQuery(loc,paste("CREATE TABLE ",schema,".",sortie_fr," AS
            select a.",n_bdgsf,",st_intersection(a.geom, b.geom) as geom
            from ",schema,".",sortie_eu," as a, dm_vecteurs.france as b
            where st_intersects(a.geom, b.geom);",sep=""))

# Suppression des fichiers temporaires
system(paste("rm ",file1," ",file2,sep=""))

# Intégration des tables de la BDGSF
Liste_dbf <- gsub("\\.dbf$","",list.files(repbdgsf,pattern="\\.dbf$"))
Liste_dbf <- Liste_dbf[4:length(Liste_dbf)]
for(i in Liste_dbf){
  dbf <- read.dbf(paste(repbdgsf,i,".dbf",sep=""))
  sqlSave(loc,dbf,tablename = paste(schema,".",i,sep=""))
}

```

### Données élaborées LUCAS

Ci-dessous, intégration des rasters dans la base postgreSQL/postGIS. Ces données ont été téchargées à cette [adresse](http://esdac.jrc.ec.europa.eu/content/topsoil-physical-properties-europe-based-lucas-topsoil-data).

```{r eval=FALSE}
rep_rasterjrc <- "/media/sf_GIS_ED/Dev/Data/Sol/ESDAC/Lucas/Elaborees/"
shaperef <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT.shp"
list.rast <- gsub("\\.tif$","",list.files(rep_rasterjrc,pattern="\\.tif$"))# Liste des xls présent dans le répertoire de travail (repagreste)

for(i in list.rast){
    # 
    system(paste("gdalwarp -t_srs EPSG:2154 ",rep_rasterjrc,i,".tif ",rep_rasterjrc,"tmp",i,".tif -overwrite",sep=""))
    system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef," -crop_to_cutline ",rep_rasterjrc,"tmp",i,".tif ",rep_rasterjrc,"fr_",i,".tif -overwrite",sep=""))
    system(paste("rm ",rep_rasterjrc,"tmp",i,".tif",sep=""))
    
    # Intégration dans la base postgis
    rpostgisname <- paste("dm_rasters.",i,sep="")
    sqlQuery(loc,paste("drop table if exists ",rpostgisname,sep=""))
    system(paste("raster2pgsql -I -C -s 2154 ",rep_rasterjrc,"fr_",i,".tif ",rpostgisname," | psql -h localhost -U jb -d sol_elevage",sep=""))
} 
```

# Les données d'occupation du sol

Les données d'occupation du sol proviennent du Recencement Agricole (RA) et de la base Corine Land Cover (CLC). Cette section présente l'intégration des données brutes dans la base de données.

## Corine Land Cover (CLC)

L'ensemble des feuilles excel du fichier `stats_clc_commune_niveau_2.xls` téléchargé à cette [adresse](http://www.statistiques.developpement-durable.gouv.fr/fileadmin/documents/Produits_editoriaux/Donnees_en_ligne/Territoires/clc-2012/stats-clc-2012-commune-niveau-2.zip) est introduit dans le schéma `clc` de la base de données. Les données représentent la surface des occupations du sol clc de niveau 2 par canton.

```{r importCLC,eval=FALSE}
# Intégration du fichier excel stats_clc_commune_niveau_2.xls
installXLSXsupport()
fichier <- paste(repCLC,"stats-clc-2012-commune-niveau-2/stats_clc_commune_niveau_2.xls",sep="")
feuilles <- c("CLC90","CHANGEMENTS90_00","CLC00_REVISEE","CHANGEMENTS00_06","CLC06_REVISEE","CLC12","CHANGEMENTS06_12")
schema <- "clc"
for(i in feuilles ){
  xlsfiles <- read.xls(fichier,sheet=i,header=TRUE,fileEncoding="latin1",sep=",")

  # Enregistrement dans la base locale
  sqlQuery(loc,paste("drop table if exists ",schema,".",i,sep=""))
  sqlSave(loc,xlsfiles,tablename = paste(schema,".",i,sep=""))
}
```

## Les données du recensement agricole (RA)

Les données du recensement agricole ont été extraites de requêtes lancées sur [Disar](https://stats.agriculture.gouv.fr/disar/). 
Dans les tables de sorties de Disard, le numéro de canton et le nom du chef lieu sont enregistrés dans le même champ. Dans les commandes suivantes, le numéro de canton et le nom du chef lieu sont dissociés dans un champ qui leur est propre pour faciliter les futures jointures.  

```{r importagreste,eval=FALSE}
installXLSXsupport()
schema <- "ra" #Nom du schema de stockage des données
catalogue <- read.csv(paste(repmetadonnees,"Catalogue_table.csv",sep=""),sep=",")
Liste_fichiers <- catalogue[(catalogue[,1] %in% schema),2]
Commentaires <- catalogue[(catalogue[,1] %in% schema),3]
#Liste_fichiers <- gsub("\\.xls$","",list.files(repagreste,pattern="\\.xls$"))# Liste des xls présent dans le répertoire de travail (repagreste)

cpt <- 0
for(i in Liste_fichiers){
  if(i=="UGBTA_canton880010"){
  # Intégration spéciale pour la table UGBTA_canton880010.csv" (Téléchargé http://agreste.agriculture.gouv.fr/IMG/xls/Donnees_principales__canton_departement_.xls)
  tablecsv <- read.csv(paste(repagreste,"UGBTA_canton880010.csv",sep=""),header=TRUE,sep=";",colClasses=c("num_canton"="character"))
  
  # Configuration des types de champs
  tablecsv[,-1] <- data.frame(lapply(tablecsv[,-1], function(v) {
    as.numeric(as.character(v))}))

  # Suppression des lignes avec une SAU=0
  tablecsv <- tablecsv[tablecsv$SAU2000>0 & tablecsv$SAU2010>0 & tablecsv$SAU1988>0,]

  # Suppression du canton 5496 (valeur abérante)
  tablecsv <- tablecsv[tablecsv$num_canton != "5496",]

  sqlQuery(loc,paste("drop table if exists ",schema,".UGBTA_canton880010",sep=""))
  sqlSave(loc,tablecsv,tablename = paste(schema,".UGBTA_canton880010",sep=""))
  
  # Ajout d'un commentaire
  print(sqlQuery(loc,paste("
	    COMMENT ON TABLE ",schema,".",i," IS \'",Comment,".\';",sep="")))
  next
  }else{}
  
  cpt <- cpt + 1 
  # Lecture du fichier
  xlsfiles <- read.xls(paste(repagreste,i,".xls",sep=""),sheet="Feuille1",header=TRUE,fileEncoding="latin1",sep=",")  
  Comment <- Commentaires[cpt]

  # Configuration des types de champs
  xlsfiles[,-1] <- data.frame(lapply(xlsfiles[,-1], function(v) {
    as.numeric(as.character(v))}))
  
  # Extraction du code canton  
  toto <- as.character(xlsfiles[[1]])
  numcanton <- regmatches(toto,gregexpr('[0-9]+.[0-9]+',toto))
  xlsfiles["Num_canton"] <- as.character(unlist(numcanton))
  
  # Extraction du nom du chef lieu (tout ce qu'il y a après le tiret)
  #regmatches(popo2,gregexpr('^[a-zA-Z]+$',popo2))
  xlsfiles["nom_chflieu"] <- gsub2(".*- ", "", as.character(unlist(toto)))
   
  # Enregistrement dans la base locale
  sqlQuery(loc,paste("drop table if exists ",schema,".",i,sep=""))
  sqlSave(loc,xlsfiles,tablename = paste(schema,".",i,sep=""))
  
  # Ajout d'un commentaire
  print(sqlQuery(loc,paste("
	    COMMENT ON TABLE ",schema,".",i," IS \'",Comment,".\';",sep="")))
}
```

## Les grandes régions d'élevage

Les grandes régions d'élevage représentent un zonage basé sur l'utilisation du sol (part de la SFP/des cultures et dans la SFP part de la STH, des PT et du maïs, et recroisement avec les zones à contraintes naturelles (montagne et zones sèches)). Le zonage a évolué depuis 1995, avec des correctifs dans certaines zones (Limousin notamment) et surtout un redécoupage en fonction de l’orientation des productions d’élevage, notamment bovin (lait ou viande) et du contexte pédoclimatique (séparer la Bretagne et les piémonts, les Bassin parisien et aquitain par exemple).

Le zonage intégré dans la base a été fourni par [Christophe Perrot](christophe.perrot@idele.fr). Pour plus d'information sur la construction, contacter l'auteur.
Pour faciliter la distinction des différents niveaux de régions, une nomenclature a été créée. Elle est consultable [ici](https://github.com/GisEDSol/Carbo_elevage/tree/master/Documentation/Metadonnees/Nomenclature_regionelevage.csv)

```{r importRegionElevage,eval=FALSE}
repRegionElevage <- "/media/sf_GIS_ED/Dev/Data/Regions_elevages/"
regelevage <- read.csv(paste(repRegionElevage,"zonage_idele_13_modif.csv",sep=""),header=TRUE,sep=";")  
schema <- "public" 

# Voir pour supprimer les valeurs avec "." (les mettres en NA)
sqlQuery(loc,paste("drop table if exists ",schema,".regelevage",sep=""))
sqlSave(loc,regelevage,tablename=paste(schema,".regelevage",sep=""))

# Jointure vers la table dm_vecteurs.commune
table_dm <- "dm_vecteurs.commune"
var <- "regelevage"

for(i in c("zonage_simple","zonage_cplt")){
  
  sqlQuery(loc,paste("alter table ",table_dm,"
                      drop column if exists ",i,sep=""))
  
  sqlQuery(loc,paste("alter table ",table_dm,"
                      add column ",i," text;
                      update ",table_dm,"
                      SET ",i," = s1.",i," from(
                      select ",i,",codecommune
                      from ",schema,".regelevage) as s1
                      where ",table_dm,".insee_com=s1.codecommune::text",sep=""))
  
  # Ajout d'un commentaire sur la nouvelle colonne créée
  print(sqlQuery(loc,paste("
  COMMENT ON COLUMN ",table_dm,".",i," IS \'",i," des principales régions d élevage. La table ",schema,".regelevage présente la signification des codes utilisés pour le zonage. Source DG AGRI RICA UE 2012 - traitement IDELE\';",sep="")))
}

# Agrégation à l'échelle du canton (valeur majoritaire)
table_dm <- "dm_vecteurs.canton"
var <- "regelevage"

for(i in c("zonage_simple","zonage_cplt")){

  # Suppression de la colonne si déjà existante
  sqlQuery(loc,paste("alter table ",table_dm,"
                    drop column if exists ",i,sep=""))
  
  # Création de la colonne, aggrégation par canton (valeur majoritaire, fonction mode()) et jointure vers la table canton 
  sqlQuery(loc,paste("alter table ",table_dm,"
                    add column ",i," text;
                    update ",table_dm,"
                    SET ",i," = s1.",i," from(
                    select (code_dept || code_cant) as num_canton,mode() within group (order by regelevage.",i,") as ",i,"
                    from ",schema,".regelevage                  
                    inner join dm_vecteurs.commune as c on c.insee_com=regelevage.codecommune
                    group by code_dept || code_cant) as s1
                    where ",table_dm,".code_canton=s1.num_canton::text",sep=""))

  #Ajout d'un commentaire sur la nouvelle colonne créée
 print(sqlQuery(loc,paste("
  COMMENT ON COLUMN ",table_dm,".",i," IS \'",i," des principales régions d élevage par canton. La valeur est issue des données communales (",schema,".regelevage) et représente la valeur majoritaire par canton. La table ",schema,".regelevage présente la signification des codes utilisés pour le zonage. Source DG AGRI RICA UE 2012 - traitement IDELE\';",sep="")))
}
```

# Les données climatiques

Plusieurs sources de données climatiques sont disponibles. Pour le moment, seul le travail de [Joly et al., 2010](http://cybergeo.revues.org/23155) est exploité. Les autres sources de données pourront être intégrées ultérieurement dans la base de données, dans le schéma `climat`.

## Topologie du climat en France

Les données climatiques utilisées proviennnent du travail de [Joly et al., 2010](http://cybergeo.revues.org/23155) sur la typologie du climat en France. Les commandes suivantes importent la table `table_commun.txt` disponible à cette [adresse](http://cybergeo.revues.org/26894?file=1). Les données climatiques sont ensuite aggrégées par canton dans le fichier de suivie [Traitement_climato.Rmd](https://github.com/GisEDSol/Carbo_elevage/blob/master/Fichiers_suivis/BDD/Suivis/FS_bdd_elab_climat.Rmd). 

```{r importClimatL,eval=FALSE}
##Paramètres
rep_climato <- paste(repdata,"Climato/Joly2010",sep="") #Répertoire des données climatiques à intégrer

#Lecture de la table
jolytable <- read.csv(paste(rep_climato,"/tableau_commun.txt",sep=""),sep="\t",header=TRUE,colClasses=c("DC"="character"),na.strings = c("-9999","2999.9"))#Dans la lecture, on spécifie le type de colonne pour DC (text) afin d'assurer les jointures. Les valeurs -9999 sont considérées comme NA. Les valeurs 2999.9 sont a priori des erreurs (valeurs abérantes)
jolytable$HPLUIE_AN <- as.numeric(as.character(jolytable$HPLUIE_AN))

# Les valeurs 9 et 0 pour la colonne typo_clim sont considérées comme NA
jolytable$TYPO_CLIM[jolytable$TYPO_CLIM==9 | jolytable$TYPO_CLIM==0] <- NA

# Correction de la valeur abérante en hauteur de pluie annuelle sur le canton 5814. Le problème vient d'une valeur extrême sur la commune 58125. La valeur est remplacée par la valeur de la commune voisine
jolytable[jolytable$DC %in% "58125","HPLUIE_AN"] <- jolytable[jolytable$DC %in% "58185","HPLUIE_AN"]

#Enregistrement vers la base, schéma climatot
sqlQuery(loc,"drop table if exists climat.climatjoly")
sqlSave(loc,jolytable,tablename="climat.climatjoly")

#Ajout d'un commentaire pour décrire la table
sqlQuery(loc,"COMMENT ON TABLE climat.climatjoly IS 'Table importée à cette adresse : http://cybergeo.revues.org/26894?file=1. Le fichier initial est au format TXT, comporte 36267 lignes (une par commune) et 16 colonnes, soit respectivement, l’identifiant communal INSEE (DC) les 14 variables climatiques traitées et une variable définissant le type climatique où se range la commune. La valeur codant l’absence de données est -9999. Plus infos : http://cybergeo.revues.org/23155 
L’usage de ces fichiers est strictement limité au domaine des services publics. Toute exploitation de la base climatique, totale ou partielle (cartes, tableaux, etc.), doit être accompagnée de la mention de la source décrite ainsi :
Source : Base de données climatiques communales 2009. THEMA Université de Franche-Comté, CNRS UMR6049 (F-25000 Besançon) /CESAER INRA UMR1041 (F-21000 Dijon) ; d’après Météo France 1971-2000.';")
```

# La topographie

## Données SRTM

Le téléchargement des données du SRTM a été réalisé sur [earthexplorer](https://earthexplorer.usgs.gov/) de la manière suivante :

- Inscription,
- Sélection la zone géographique (search criteria >> Use Map). **Attention,** le nombre de tuile est limité à 100. Bien sélectionner la zone désirée ou le faire en deux fois.
- Sélection du type de données souhaité (Data Sets >> Digital Elevation >> SRTM >> SRTM 1 Arc-Second Global),
- Cliquer sur 'Results' et sélectionner l'ensemble des tuiles à télécharger (add to bulk download >> submit standing request),
- Utilisation du logiciel [BDT](https://earthexplorer.usgs.gov/bulk) pour télécharger les tuiles commandées. 

**Attention**, il est nécessaire d'ouvrir les ports 4448 pour utiliser le logiciel.


```{r,eval=FALSE}
# Décompression de tous les fichiers zip présent dans le répertoire repSRTM
repSRTM <- "/media/sf_GIS_ED/Dev/Data/Topographie/SRTM/SRTM_1_Arc_Second_Global/"
#system(paste("unzip ",repSRTM,"'*.zip' -d ",repSRTM,sep=""))

shaperef3035 <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT.shp"
vrtname <- paste(repSRTM,"mergebil.vrt",sep="")
file1tmp <- paste(repSRTM,"tmp1.tif",sep="")
file2tmp <- paste(repSRTM,"tmp2.tif",sep="")
srtmName <- paste(repSRTM,"Fr_L93_srtm.tif",sep="") #Nom du raster final

# Liste des tuiles à assembler
listbil <- paste(repSRTM,list.files(repSRTM,pattern="\\.bil$"),sep="",collapse=" ")#gsub("\\.bil$","", list.files(repSRTM,pattern="\\.bil$"))

# Création d'un raster virtuel pour assembler les rasters (plus rapide que gdal_merge.py)
system(paste("gdalbuildvrt ",vrtname," ",listbil,sep=""))

# Conversion au format tif
system(paste("gdal_translate -co TILED=YES -co 'COMPRESS=LZW' -co BIGTIFF=YES -a_srs EPSG:4326 ",vrtname," ",file1tmp,sep=""))#testvrt.tif

# Reprojection
system(paste("gdalwarp -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",file1tmp, " ",file2tmp," -overwrite",sep=""))

# Découpage 
system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef2154," -crop_to_cutline -co 'COMPRESS=LZW' ",file2tmp," ",srtmName," -overwrite",sep=""))

# Création de pyramides pour faciliter la visualisation
systemp(paste("gdaladdo -r average ",srtmName," 2 4 8 16",sep=""))

# Suppression des fichiers temporaires
system(paste("rm ",file1tmp," ",file2tmp,sep=""))
```

## Données européennes

Le modèle numérique de terrain provient du programme Européen Copernic. Les données ont été téléchargées à cette [adresse](http://www.eea.europa.eu/data-and-maps/data/eu-dem). Pour l'exploitation des données, le fichier au format GeoTIFF est découpé aux contours de la France et projeté en Lambert 93 (EPSG 2154). Le raster est ensuite intégré dans le serveur postgis. Ces procédures sont réalisées avec les fonctions `gdal`. Les statistiques calculées avec ces données sont réalisées dans le fichier de suivi 
[FS_bdd_elab_topo.Rmd](https://github.com/GisEDSol/Carbo_elevage/blob/master/Fichiers_suivis/BDD/Suivis/FS_bdd_elab_topo.Rmd).

Les tuiles téléchargées sont les suivantes : E30N20; E40N20;E30N30. Les instructions suivantes permettent d'assembler les 3 rasters, de les découper aux contours de la France et de projeter le dernier fichier en Lambert 93 (EPSG 2154).

```{r,eval=FALSE}
reptopo <- "/media/sf_GIS_ED/Dev/Data/Topographie/dem_copernicus/" #répertoire contenant les rasters à assembler
shaperef3035 <- "/media/sf_GIS_ED/Dev/Data/Base/DEPARTEMENT_3035.shp" #Shapefiles utilisé pour le découpage aux contours de la France
listtif <- paste(reptopo,c("eu_dem_v11_E40N20.tif","eu_dem_v11_E30N30.tif","eu_dem_v11_E30N20.tif"),sep="",collapse=" ")
vrtname <- "europ_dem.vrt"
file1tmp <- paste(reptopo,"tmp1.tif",sep="")
file2tmp <- paste(reptopo,"tmp2.tif",sep="")
Fr_demName <- paste(reptopo,"Fr_L93_eudem.tif",sep="")
Fr_demNamelight <- paste(reptopo,"Fr_L93_90eudem.tif",sep="")

# Création d'un raster virtuel pour assembler les rasters (plus rapide que gdal_merge.py)
system(paste("gdalbuildvrt ",reptopo,vrtname," ",listtif,sep=""))

# Conversion au format tiff
system(paste("gdal_translate -co TILED=YES -co 'COMPRESS=LZW' -co BIGTIFF=YES -a_srs EPSG:3035 ",reptopo,vrtname," ",file1tmp,sep=""))#testvrt.tif

# Découpage aux contours de la France
system(paste("gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -cutline ",shaperef3035," -crop_to_cutline -co TILED=YES -co BIGTIFF=YES -co 'COMPRESS=LZW' ",file1tmp," ",file2tmp," -overwrite",sep=""))

# Reprojection en Lambert 93
system(paste("gdalwarp -co BIGTIFF=YES -co TILED=YES -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",file2tmp," ",Fr_demName," -overwrite",sep=""))

# Création de pyramides pour faciliter la visualisation
systemp(paste("gdaladdo -r average ",Fr_demName," 2 4 8 16",sep=""))

# Nettoyage des fichiers temporaires
system(paste("rm ",file1tmp," ",file2tmp,sep=""))

# Création d'un raster plus léger pour les analyses
system(paste("gdalwarp -tr 90 90 -co BIGTIFF=YES -co TILED=YES -co 'COMPRESS=LZW' -t_srs EPSG:2154 ",Fr_demName," ",Fr_demNamelight," -overwrite",sep=""))
```

Le raster est ensuite intégré sur le serveur PostGIS.

```{r demtopostgis,eval=FALSE}
rastername <- Fr_demNamelight
rpostgisname <- "dm_rasters.dem_copernicus_90"

sqlQuery(loc,paste("drop table if exists ",rpostgisname,sep=""))
system(paste("raster2pgsql -I -C -s 2154 ",rastername," ",rpostgisname," | psql -h localhost -U jb -d sol_elevage",sep=""))

# Ajout d'un commentaire
print(sqlQuery(loc,paste("
      COMMENT ON TABLE ",rpostgisname," IS \'Modèle numérique de terrain du programme Copernicus (résolution 90m).\';",sep="")))
```

# Les métadonnées

## Création de métadonnées pour les schémas

La description des schémas de la base de données est stockée dans le fichier [Catalogue_schema.csv](https://github.com/GisEDSol/Carbo_elevage/tree/master/Documentation/Metadonnees/Catalogue_schema.csv) et ces commentaires sont intégrés dans la base de données avec la commande suivante :

```{r,eval=FALSE}
meta_schema <- read.csv(paste(repmetadonnees,"/Catalogue_schema.csv",sep=""),sep=",",header=TRUE)
for(i in 1:nrow(meta_schema)){
  sqlQuery(loc,paste("COMMENT ON SCHEMA ",meta_schema[i,1]," IS '",meta_schema[i,2],"'",sep=""))
}
```

## Création d'une vue pour les métadonnées

Pour faciliter la compréhension des données stockées dans la base de données, une vue des métadonnées est générée à travers la commande suivante. Celle-ci est accessible sur `public.metadata`

```{r,eval=FALSE}
# Lancement du script sql pour créer une vue sur les métadonnées (accessible sur public.metadata)
system(paste("sh ",repfonctions,"bash/create_view_metada.sh",sep=""))
```
# Export de la base

Deux formats sont proposés pour exploiter la base de données :

- une base SQLite/Spatialite sous forme de fichier `.sqlite`,
- une sauvegarde de la base PostgreSQL/PostGIS sous forme de fichier texte (`sql`).

## Export vers SQLite/Spatialite

L'export de la base sous forme SQLite/Spatialite offre l'opportunité d'exploiter la base rapidement à travers l'ouverture du fichier directement connectable avec des clients de données géographiques (comme QGIS) ou de base de données (sqlitebrower). Lors de la conversion, l'arborescence des tables n'est pas maintenue et les commentaires des champs faisant office de métadonnées ont disparu.
Le fichier est néanmoins utile pour visualiser rapidement les données géographiques.
Rajouter lien vers des modes opératoires pour lire les données vers qgis.

```{r,eval=FALSE}
# Vérification (selon http://gis.stackexchange.com/questions/168819/fixing-ogr2ogr-without-spatialite-support)
ogrinfo --formats sqlite | grep 'spatialite' -i
ogrinfo --format sqlite | grep 'spatialite' -i

system("ogr2ogr --config PG_LIST_ALL_TABLES YES --config PG_SKIP_VIEWS YES -f "SQLite" sol_elevage.sqlite -progress PG:"dbname='sol_elevage' host='localhost' port='5432' user='jb' password='170284'" -lco LAUNDER=yes -dsco SPATIALITE=yes -lco SPATIAL_INDEX=yes -gt 65536")
```

## Export de la base en PostgreSQL/PostGIS

La base de données est sauvegardée au format PostgreSQL/PostGIS avec la commande `pg_dump`. Cette commande permet de garder l'ensemble de la base de données mais pour être restaurer, l'utilisateur doit créer un serveur local Postgresql/PostGIS en suivant la procédure présentée dans ce [mode opératoire](https://rawgit.com/GisEDSol/Carbo_elevage/master/Documentation/Modes_operatoires/MO_bdd.html).

Voir le problème avec sur les postgis sur ce lien http://mattmakesmaps.com/blog/2014/01/15/using-pg-dump-with-postgis-topology/

```{r,eval=FALSE}
# Export avec pg_dump
system(paste("pg_dump sol_elevage > sol_elevage",Sys.Date(),".sql",sep=""))

# En cas de problème
#select AddGeometryColumn('dm_vecteurs','canton','geom','2154','MULTIPOLYGON',2)
#update dm_vecteurs.canton set geom=s1.geom from(
#select geom,id_geofla
#from dm_vecteurs.tmpcanton) s1
#where dm_vecteurs.canton.id_geofla= s1.id_geofla
```